# -*- coding: utf-8 -*-
"""Credit Card Fraud Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WYym6BdYJL77OV-Fq6jVpG2iQtalOjtd

**Credit Card Fraud Detection**
"""

#Importing the libraries
import numpy as np
import sklearn
from sklearn.utils import resample
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.model_selection import cross_validate
import warnings
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")

"""**Read Test Data Set**"""

# Load test data
dataset_test = pd.read_csv('fraudTest.csv')
dataset_test.head(5)

"""**Read Train Data Set**"""

# Load training data
dataset_train = pd.read_csv('fraudTrain.csv')
dataset_train.head(5)

"""**Preprocessing**"""

# Size of both dataframes
print(f"The shape of train set: {dataset_train.shape}")
print(f"Test shape of test set: {dataset_test.shape}")

dataset_train.info()

# Number of null values in each column
dataset_train.isnull().sum()

dataset_test.info()

# Number of null values in each column
dataset_test.isnull().sum()

"""**Cleaning**"""

#Removing non-important columns
def clean_dataset(clean):
  clean.drop(["Unnamed: 0",'cc_num','first', 'last', 'street', 'city', 'state', 'zip', 'dob', 'trans_num','trans_date_trans_time'],axis=1, inplace=True)
  clean.dropna()
  return clean

# Function to call training data
clean_dataset(dataset_train)

# Function to call test data
clean_dataset(dataset_test)

# Columns with categorical values
dataset_train.select_dtypes(include = ['object'])

#converting categorical data into numerical data
encoder=LabelEncoder()
def encode(data):
    data['merchant']=encoder.fit_transform(data['merchant'])
    data["category"] = encoder.fit_transform(data["category"])
    data["gender"] = encoder.fit_transform(data["gender"])
    data["job"] = encoder.fit_transform(data["job"])
    return data

#Function to call train and test data
encode(dataset_train)
encode(dataset_test)

"""**Visualize data**"""

from matplotlib import pyplot as plt
exit_counts = dataset_train["is_fraud"].value_counts()
plt.figure(figsize=(7, 7))
plt.subplot(1, 2, 1)  # Subplot for the pie chart
plt.pie(exit_counts, labels=["No", "YES"], autopct="%0.0f%%")
plt.title("is_fraud Counts")
plt.tight_layout()  # Adjust layout to prevent overlapping
plt.show()

import seaborn as sns
pd.options.display.float_format = "{:,.2f}".format

corr_matrix = dataset_train.corr(method = 'pearson')

mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
corr_matrix[(corr_matrix < 0.3) & (corr_matrix > -0.3)] = 0

cmap = "mako"

# the heatmap
sns.heatmap(corr_matrix, mask=mask, vmax=1.0, vmin=-1.0, linewidths=0.1,
            annot_kws={"size": 9, "color": "black"}, square=True, cmap=cmap, annot=True)

"""**Split Data and Train Model**"""

x=dataset_train.drop(columns=['is_fraud'])
y=dataset_train['is_fraud']

x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.2, random_state=42)

model1 = LogisticRegression()
model2 = RandomForestClassifier()
model3 = DecisionTreeClassifier()
model4 = KNeighborsClassifier()
model5 = XGBClassifier()

from sklearn.metrics import accuracy_score

# Define a function for each metric
def acc_score(test, pred):
    acc_ = accuracy_score(test, pred)
    return acc_

# Print the scores
def print_score(test, pred, model):

    print(f"Classifier: {model}")
    print(f"ACCURACY: {accuracy_score(test, pred)}")

"""**Logistic Regression**"""

model1.fit(x_train,y_train)

y_pred = model1.predict(x_test)

print_score(y_test, y_pred, "Logistic Regression")

model_list = []
acc_list = []

model_list.append(model1.__class__.__name__)
acc_list.append(round(acc_score(y_test, y_pred), 4))

"""**Random Forest**"""

model2.fit(x_train,y_train)

y_pred1 = model2.predict(x_test)

print_score(y_test,y_pred1,"Random Forest")

model_list.append(model2.__class__.__name__)
acc_list.append(round(acc_score(y_test, y_pred), 4))

"""**Decision Trees**"""

model3.fit(x_train,y_train)

y_pred2 = model3.predict(x_test)

print_score(y_test, y_pred2, "Decision Tree")

model_list.append(model3.__class__.__name__)
acc_list.append(round(acc_score(y_test, y_pred2), 4))

"""**KNeighbour**"""

model4.fit(x_train,y_train)

y_pred3 = model4.predict(x_test)

print_score(y_test, y_pred3, "KNeighbour")

model_list.append(model4.__class__.__name__)
acc_list.append(round(acc_score(y_test, y_pred3), 4))

"""**XGBClassifier**"""

model5.fit(x_train,y_train)

y_pred4 = model5.predict(x_test)

print_score(y_test, y_pred4, "XGBClassifier")

model_list.append(model5.__class__.__name__)
acc_list.append(round(acc_score(y_test, y_pred4), 4))

"""**Comparison and Results**"""

print("Model List:", model_list)
print("Accuracy List:", acc_list)

# Remove duplicates while preserving order
unique_models = []
unique_acc = []
for model, acc in zip(model_list, acc_list):
    if model not in unique_models:
        unique_models.append(model)
        unique_acc.append(acc)

# Now unique_models and unique_acc should have the same length
model_results = pd.DataFrame({"Model": unique_models, "Accuracy_Score": unique_acc})

model_results